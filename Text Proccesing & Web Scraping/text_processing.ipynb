{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing and installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "nltk.download('punkt') # tokenizer package \n",
    "nltk.download('stopwords') # package containing the stopwords\n",
    "nltk.download('wordnet') # wordnet package\n",
    "nltk.download('omw-1.4') # open multilingual wordnet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regex for operations on the data\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing the file having the text to be used for text processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (change the file name as per your need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('golden_ratio.txt', 'r')# here r means the file is opened in reading mode, that is we cannot make changes to the file\n",
    "corpus = f.read()  # this will read the contents of the file\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the text to lower case\n",
    "lower_case = corpus.lower()\n",
    "print(\"-------------------------\\n THE CORPUS IN LOWER CASE \\n-------------------------\\n\", lower_case)\n",
    "\n",
    "\n",
    "#removing everything from the corpus except letters and numbers\n",
    "removed_punctuation = re.sub(r\"[^a-zA-Z0-9]\",\" \", lower_case) # this mean remove everything except a-z A-Z and 0-9 and replace them with a space\n",
    "print(\"-------------------------------------------\\n THE CORPUS AFTER REMOVING THE PUNCTUATIONS\\n-------------------------------------------\\n\",removed_punctuation1)\n",
    "\n",
    "\n",
    "# OR instead of regex we can manually remove the punctuations as\n",
    "punctuations = string.punctuation\n",
    "for punctuation in punctuations:\n",
    "    removed_punctuation1= lower_case.replace(punctuation,\"\")\n",
    "print(\"---------------------------------------------------------------\\nTHE CORPUS AFTER REMOVING EVERYTHING EXCEPT LETTERS AND NUMBERS\\n---------------------------------------------------------------\\n\",removed_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting using python (this will split the strings into a string based on spaces, tabs and new line)\n",
    "words_list = removed_punctuation.split()\n",
    "print(\"--------------------------\\nTHE WORDS LIST USING SPLIT\\n--------------------------\\n\",words_list)\n",
    "\n",
    "# splitting using nltk command ( gives better output )\n",
    "words_list1 = word_tokenize(removed_punctuation, language='english', preserve_line=False)\n",
    "print(\"---------------------------\\nWORDS USING NLTK TOKENIZER\\n---------------------------\\n\", words_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing the stop words from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words are the frequently occuring words in a corpus which don't contribute much to the overall meaning of the text\n",
    "stop_words=stopwords.words(\"english\")\n",
    "print(\"----------\\nSTOP WORDS\\n----------\\n\", stop_words)\n",
    "\n",
    "removed_stopWords = [not_stop for not_stop in words_list1 if not_stop not in stop_words]\n",
    "print(\"------------------------------------\\nCORPUS AFTER REMOVING THE STOP WORDS\\n------------------------------------\\n\", removed_stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming & Lemmatization of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
    "# Basically what stemming does is, it crudely cuts the inflectional part of the words\n",
    "stemmed=[]\n",
    "for word in removed_stopWords:\n",
    "    stemmed.append(PorterStemmer().stem(word))\n",
    "print(\"--------------------\\nWORDS AFTER STEMMING\\n--------------------\\n\", stemmed)\n",
    "\n",
    "\n",
    "# It is also kindof similar to stemming but gives better results as it takes much more factors into consideration and uses a dictionary to get the base form of a word\n",
    "lemmed=[]\n",
    "for word in removed_stopWords:\n",
    "    lemmed.append(WordNetLemmatizer().lemmatize(word))\n",
    "print(\"-------------------------\\nWORDS AFTER LEMMATIZATION\\n-------------------------\\n\", lemmed)\n",
    "\n",
    "\n",
    "# Lemmatization clearly identifies the base form of 'troubled' to 'trouble'' denoting some meaning whereas,\n",
    "#  Stemming will cut out 'ed' part and convert it into 'troubl' which has the wrong meaning and spelling errors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now the text is ready for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close() #to close the file in python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
